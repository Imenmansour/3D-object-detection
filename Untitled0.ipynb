{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ogQ4uiGP2Mgr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "# Extract radar point cloud data\n",
    "from nuscenes.utils.data_classes import RadarPointCloud\n",
    "from nuscenes.utils.geometry_utils import transform_matrix\n",
    "from typing import List\n",
    "import os\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import RadarPointCloud, Box\n",
    "from pyquaternion import Quaternion\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import RadarPointCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory_path='data/sets/nuscenes'\n",
    "# os.makedirs(directory_path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhLjyWG02AML",
    "outputId": "de69b6eb-63cb-4b41-a075-1ea3df374dc5"
   },
   "outputs": [],
   "source": [
    "# !wget https://www.nuscenes.org/data/v1.0-mini.tgz -P  # Download the nuScenes mini split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xf v1.0-mini.tgz -C ./data/sets/nuscenes  # Uncompress the nuScenes mini split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-XjF-3B2Pbs",
    "outputId": "420dd2e0-538a-4af0-d5ec-fd8ecfa22596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 1.252 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "nusc = NuScenes(version='v1.0-mini',dataroot='./data/sets/nuscenes',verbose=True)\n",
    "\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMbYHKFM2Rx2",
    "outputId": "d9a8f0f5-4b8e-457e-b18c-f57fb8fc3a8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.600 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "# Initialize NuScenes dataset\n",
    "root_dir = './data/sets/nuscenes'\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=root_dir, verbose=True)\n",
    "\n",
    "# Split scene tokens\n",
    "scene_tokens = [scene['token'] for scene in nusc.scene]\n",
    "\n",
    "# Define the number of scenes for each split\n",
    "num_train = int(0.7 * len(scene_tokens))\n",
    "num_val = int(0.2 * len(scene_tokens))\n",
    "num_test = len(scene_tokens) - num_train - num_val\n",
    "\n",
    "# Create splits\n",
    "train_scenes = scene_tokens[:num_train]\n",
    "val_scenes = scene_tokens[num_train:num_train+num_val]\n",
    "test_scenes = scene_tokens[num_train+num_val:]\n",
    "\n",
    "# Write to files\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(root_dir, 'train_scenes.txt'), 'w') as f:\n",
    "    for scene_token in train_scenes:\n",
    "        f.write(f\"{scene_token}\\n\")\n",
    "\n",
    "with open(os.path.join(root_dir, 'val_scenes.txt'), 'w') as f:\n",
    "    for scene_token in val_scenes:\n",
    "        f.write(f\"{scene_token}\\n\")\n",
    "\n",
    "with open(os.path.join(root_dir, 'test_scenes.txt'), 'w') as f:\n",
    "    for scene_token in test_scenes:\n",
    "        f.write(f\"{scene_token}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TnOcC5R_2U9y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lB3qR9NG2XLc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import math\n",
    "import numpy as np\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import RadarPointCloud\n",
    "from nuscenes.utils.geometry_utils import transform_matrix\n",
    "from pyquaternion import Quaternion\n",
    "import os\n",
    "import random\n",
    "import os.path as osp\n",
    "from torch.utils.data import Dataset\n",
    "from nuscenes.utils.data_classes import RadarPointCloud, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "88JcFHf62Z_r"
   },
   "outputs": [],
   "source": [
    "def resize(image, size):\n",
    "    image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mDxT7C3S2cpY"
   },
   "outputs": [],
   "source": [
    "class NuScenesRadarDataset(Dataset):\n",
    "    def __init__(self, root_dir, version, set, radar_channel):\n",
    "        self.root_dir = root_dir\n",
    "        self.version = version\n",
    "        self.set = set\n",
    "        self.radar_channel = radar_channel\n",
    "        self.batch_count = 0\n",
    "        self.nusc = NuScenes(version=self.version, dataroot=self.root_dir, verbose=True)\n",
    "        self.scene_tokens = self._load_scene_tokens()\n",
    "        self.sample_tokens = self.get_samples_for_split(self.set)\n",
    "\n",
    "        # Define class labels and mapping\n",
    "        self.class_list = [\n",
    "            'pedestrian','vehicle', 'animal'\n",
    "        ]\n",
    "        self.map_name_from_general_to_detection = {\n",
    "            'human.pedestrian.adult': 'pedestrian',\n",
    "            'human.pedestrian.child': 'pedestrian',\n",
    "            'human.pedestrian.wheelchair': 'pedestrian',\n",
    "            'human.pedestrian.stroller': 'pedestrian',\n",
    "            'human.pedestrian.personal_mobility': 'pedestrian',\n",
    "            'human.pedestrian.police_officer': 'pedestrian',\n",
    "            'human.pedestrian.construction_worker': 'pedestrian',\n",
    "            'animal': 'animal',\n",
    "            'vehicle.car': 'vehicle',\n",
    "            'vehicle.motorcycle': 'vehicle',\n",
    "            'vehicle.bicycle': 'vehicle',\n",
    "            'vehicle.bus.bendy': 'vehicle',\n",
    "            'vehicle.bus.rigid': 'vehicle',\n",
    "            'vehicle.truck': 'vehicle',\n",
    "            'vehicle.construction': 'vehicle',\n",
    "            'vehicle.emergency.ambulance': 'vehicle',\n",
    "            'vehicle.emergency.police': 'vehicle',\n",
    "            'vehicle.trailer': 'vehicle',\n",
    "            'movable_object.barrier': 'vehicle',\n",
    "            'movable_object.trafficcone': 'vehicle',\n",
    "            'movable_object.pushable_pullable': 'vehicle',\n",
    "            'movable_object.debris': 'vehicle',\n",
    "            'static_object.bicycle_rack': 'vehicle',\n",
    "        }\n",
    "\n",
    "    def _load_scene_tokens(self):\n",
    "        split_file = os.path.join(self.root_dir, f'{self.set}_scenes.txt')\n",
    "        with open(split_file, 'r') as f:\n",
    "            scene_tokens = [line.strip() for line in f.readlines()]\n",
    "        return scene_tokens\n",
    "\n",
    "    def get_samples_for_split(self, split):\n",
    "        # Load scene tokens from split files (train, val, test)\n",
    "        scene_splits = {\n",
    "            'train': 'train_scenes.txt',\n",
    "            'val': 'val_scenes.txt',\n",
    "            'test': 'test_scenes.txt'\n",
    "        }\n",
    "        split_file = os.path.join(self.root_dir, scene_splits[split])\n",
    "        with open(split_file, 'r') as f:\n",
    "            scene_tokens = f.read().splitlines()\n",
    "\n",
    "        samples = []\n",
    "        for scene_token in scene_tokens:\n",
    "            scene = self.nusc.get('scene', scene_token)\n",
    "            sample_token = scene['first_sample_token']\n",
    "            while sample_token:\n",
    "                samples.append(sample_token)\n",
    "                sample = self.nusc.get('sample', sample_token)\n",
    "                sample_token = sample['next']\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_tokens)\n",
    "    def resize(self, image, size):\n",
    "        \"\"\"Resize the image to the specified size.\"\"\"\n",
    "        # Convert NumPy array to PyTorch tensor before applying 'unsqueeze'\n",
    "        image = torch.from_numpy(image)\n",
    "        image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample_token = self.sample_tokens[index]\n",
    "        sample = self.nusc.get('sample', sample_token)\n",
    "        sample_data_token = sample['data'][self.radar_channel]\n",
    "\n",
    "        # Transform radar point cloud to global coordinates\n",
    "        radar_points = self.transform_pc_to_global(sample_data_token)\n",
    "            # Calculate and print boundaries\n",
    "        boundaries = self.calculate_boundaries(radar_points)\n",
    "        # print(f\"xmin: {boundaries['minX']}, xmax: {boundaries['maxX']}\")\n",
    "        # print(f\"ymin: {boundaries['minY']}, ymax: {boundaries['maxY']}\")\n",
    "\n",
    "\n",
    "        # Calculate BEV image\n",
    "        bev_shape = (608, 608)  # Example BEV shape\n",
    "\n",
    "        bev_image = self.create_bev_grid(radar_points, bev_shape)\n",
    "        rgb_map_normalized = self.normalize_rgb_map(bev_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Get ground truth boxes and labels\n",
    "        targets = self.get_gt_radar(radar_points,sample)\n",
    "        ntargets = 0\n",
    "        for i, t in enumerate(targets):\n",
    "            if t.sum(0):\n",
    "                ntargets += 1\n",
    "\n",
    "        # Initialize a torch tensor for targets with the correct shape\n",
    "        target_tensor = torch.zeros((ntargets, 8))\n",
    "\n",
    "        for i, t in enumerate(targets):\n",
    "            if t.sum(0):\n",
    "                target_tensor[i, 1:] = torch.from_numpy(t)\n",
    "\n",
    "        return rgb_map_normalized, target_tensor\n",
    "\n",
    "\n",
    "    def transform_pc_to_global(self, sample_data_token):\n",
    "        sample_data = self.nusc.get('sample_data', sample_data_token)\n",
    "        ego_pose = self.nusc.get('ego_pose', sample_data['ego_pose_token'])\n",
    "        cal_sensor = self.nusc.get('calibrated_sensor', sample_data['calibrated_sensor_token'])\n",
    "\n",
    "        # Load radar point cloud\n",
    "        pc = RadarPointCloud.from_file(osp.join(self.nusc.dataroot, sample_data['filename']))\n",
    "\n",
    "        # Get transformation matrices\n",
    "        car_from_sensor = transform_matrix(cal_sensor['translation'], Quaternion(cal_sensor['rotation']), inverse=False)\n",
    "        global_from_car = transform_matrix(ego_pose['translation'], Quaternion(ego_pose['rotation']), inverse=False)\n",
    "\n",
    "        # Transform points to global coordinates\n",
    "        global_from_sensor = np.dot(global_from_car, car_from_sensor)\n",
    "        pc.transform(global_from_sensor)\n",
    "\n",
    "        return pc.points\n",
    "    def normalize_rgb_map(self, rgb_map):\n",
    "            # Normalize only the blue channel\n",
    "            min_val_blue = np.min(rgb_map[0])\n",
    "            max_val_blue = np.max(rgb_map[0])\n",
    "\n",
    "            if max_val_blue == min_val_blue:\n",
    "                rgb_map[0] = np.zeros_like(rgb_map[0])  # Prevent division by zero\n",
    "            else:\n",
    "                rgb_map[0] = (rgb_map[0] - min_val_blue) / (max_val_blue - min_val_blue)\n",
    "\n",
    "            return rgb_map\n",
    "\n",
    "\n",
    "\n",
    "    def create_bev_grid(self, radar_points, bev_shape):\n",
    "        Height, Width = bev_shape[0]+1, bev_shape[1]+1\n",
    "\n",
    "        # Calculate boundaries based on radar points\n",
    "        boundaries = self.calculate_boundaries(radar_points)\n",
    "        x_min, x_max = boundaries[\"minX\"], boundaries[\"maxX\"]\n",
    "        y_min, y_max = boundaries[\"minY\"], boundaries[\"maxY\"]\n",
    "\n",
    "        # Calculate extent in meters\n",
    "        extent_x = x_max - x_min\n",
    "        extent_y = y_max - y_min\n",
    "\n",
    "        # Calculate resolution\n",
    "        resolution_x = extent_x / Width  # Width dimension\n",
    "        resolution_y = extent_y / Height  # Height dimension\n",
    "\n",
    "        # Discretize radar points\n",
    "        radar_points_copy = np.copy(radar_points)\n",
    "        radar_points_copy[0, :] = np.int_(np.floor((radar_points_copy[0, :] - x_min) / resolution_x))\n",
    "        radar_points_copy[1, :] = np.int_(np.floor((radar_points_copy[1, :] - y_min) / resolution_y))\n",
    "\n",
    "        # Ensure that the coordinates are within the bounds of the BEV grid\n",
    "        radar_points_copy[0, :] = np.clip(radar_points_copy[0, :], 0, Width - 1)\n",
    "        radar_points_copy[1, :] = np.clip(radar_points_copy[1, :], 0, Height - 1)\n",
    "\n",
    "        # Sort the radar points\n",
    "        indices = np.lexsort((radar_points_copy[1, :], radar_points_copy[0, :]))\n",
    "        radar_points_copy = radar_points_copy[:, indices]\n",
    "\n",
    "        # Position Map (use x and y coordinates)\n",
    "        positionMap = np.zeros((Height, Width))\n",
    "\n",
    "        _, unique_indices = np.unique(radar_points_copy[0:2, :], axis=1, return_index=True)\n",
    "        radar_points_frac = radar_points_copy[:, unique_indices]\n",
    "\n",
    "        # Mark the presence of a point in the BEV map grid cell\n",
    "        positionMap[np.int_(radar_points_frac[0, :]), np.int_(radar_points_frac[1, :])] = 1\n",
    "\n",
    "        # Intensity Map (use rcs)\n",
    "        intensityMap = np.zeros((Height, Width))\n",
    "        densityMap = np.zeros((Height, Width))\n",
    "\n",
    "        _, unique_indices, counts = np.unique(radar_points_copy[0:2, :], axis=1, return_index=True, return_counts=True)\n",
    "        radar_points_top = radar_points_copy[:, unique_indices]\n",
    "\n",
    "        normalizedCounts = np.minimum(1.0, np.log(counts + 1) / np.log(64))\n",
    "\n",
    "        intensityMap[np.int_(radar_points_top[0, :]), np.int_(radar_points_top[1, :])] = radar_points_top[5, :]  # RCS\n",
    "        densityMap[np.int_(radar_points_top[0, :]), np.int_(radar_points_top[1, :])] = normalizedCounts\n",
    "\n",
    "        # RGB Map: Create a composite RGB map from the 3 individual maps\n",
    "        RGB_Map = np.zeros((3, Height-1, Width-1))\n",
    "        RGB_Map[2, :, :] = densityMap[:bev_shape[0], :bev_shape[0]]   # Density Map (Red channel)\n",
    "        RGB_Map[1, :, :] = positionMap[:bev_shape[0], :bev_shape[0]]  # Position Map (Green channel)\n",
    "        RGB_Map[0, :, :] = intensityMap[:bev_shape[0], :bev_shape[0]] # Intensity Map (Blue channel)\n",
    "\n",
    "        # # Print the range of values in the maps\n",
    "        # print(\"Position Map Range:\")\n",
    "        # print(f\"Min: {np.min(positionMap)}, Max: {np.max(positionMap)}\")\n",
    "\n",
    "        # print(\"Intensity Map Range:\")\n",
    "        # print(f\"Min: {np.min(intensityMap)}, Max: {np.max(intensityMap)}\")\n",
    "\n",
    "        # print(\"Density Map Range:\")\n",
    "        # print(f\"Min: {np.min(densityMap)}, Max: {np.max(densityMap)}\")\n",
    "\n",
    "        return RGB_Map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_boundaries(self, radar_data):\n",
    "        x = radar_data[0, :]\n",
    "        y = radar_data[1, :]\n",
    "\n",
    "        min_x = np.min(x)\n",
    "        max_x = np.max(x)\n",
    "        min_y = np.min(y)\n",
    "        max_y = np.max(y)\n",
    "\n",
    "        return {\n",
    "            \"minX\": min_x,\n",
    "            \"maxX\": max_x,\n",
    "            \"minY\": min_y,\n",
    "            \"maxY\": max_y\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_gt_radar(self, radar_points, sample):\n",
    "        boundaries = self.calculate_boundaries(radar_points)\n",
    "\n",
    "        # Initialize the target array with a fixed shape [50, 7]\n",
    "        target = np.zeros([50, 7], dtype=np.float32)\n",
    "\n",
    "        index = 0\n",
    "        for ann_token in sample['anns']:\n",
    "            ann_info = self.nusc.get('sample_annotation', ann_token)\n",
    "\n",
    "            if ann_info['num_radar_pts'] > 0:\n",
    "                if ann_info['category_name'] in self.map_name_from_general_to_detection:\n",
    "                    category_name = ann_info['category_name']\n",
    "                    label = self.map_name_from_general_to_detection[category_name]\n",
    "\n",
    "                    # Create bounding box\n",
    "                    box = Box(\n",
    "                        ann_info['translation'],\n",
    "                        ann_info['size'],\n",
    "                        Quaternion(ann_info['rotation'])\n",
    "                    )\n",
    "                    box_center = np.array(box.center)\n",
    "                    box_dims = np.array(box.wlh)  # width, length, height\n",
    "\n",
    "                    # Normalize coordinates\n",
    "                    x_normalized = (box_center[0] - boundaries['minX']) / (boundaries['maxX'] - boundaries['minX'])\n",
    "                    y_normalized = (boundaries['maxY'] - box_center[1]) / (boundaries['maxY'] - boundaries['minY'])\n",
    "\n",
    "                    # Normalize width and height\n",
    "                    width_normalized = box_dims[0] / (boundaries['maxX'] - boundaries['minX'])\n",
    "                    height_normalized = box_dims[2] / (boundaries['maxY'] - boundaries['minY'])\n",
    "\n",
    "                    \n",
    "\n",
    "                    yaw = box.orientation.yaw_pitch_roll[0]\n",
    "                    yaw_sin = math.sin(float(yaw))\n",
    "                    yaw_cos = math.cos(float(yaw))\n",
    "\n",
    "                    # Adjust width and height if necessary\n",
    "                    if box_dims[0] < 1.0:  # Example adjustment for small width\n",
    "                        width_normalized += 0.3 / (boundaries['maxX'] - boundaries['minX'])\n",
    "                    if box_dims[2] < 1.0:  # Example adjustment for small height\n",
    "                        height_normalized += 0.3 / (boundaries['maxY'] - boundaries['minY'])\n",
    "\n",
    "                    if 0<= x_normalized <=1 and 0<= y_normalized <=1:                  \n",
    "                        if index < 50:  # Ensure index does not exceed the fixed target array size\n",
    "                            target[index][0] = self.class_list.index(label)  # Class ID\n",
    "                            target[index][1] = y_normalized  # y coordinate\n",
    "                            target[index][2] = x_normalized  # x coordinate\n",
    "                            target[index][3] = width_normalized  # Width\n",
    "                            target[index][4] = height_normalized  # Height\n",
    "                            target[index][5] = yaw_sin  # Yaw Sin\n",
    "                            target[index][6] = yaw_cos  # Yaw Cos\n",
    "    \n",
    "                            index += 1\n",
    "\n",
    "        # Print target array shape and contents for debugging\n",
    "        # print(f\"Target shape: {target.shape}\")\n",
    "        # print(f\"Target contents: {target}\")\n",
    "\n",
    "        # Return the target array with accumulated targets\n",
    "        return target\n",
    "    def collate_fn(self, batch):\n",
    "        # Unpack batch elements\n",
    "        imgs, targets = zip(*batch)\n",
    "\n",
    "        # Remove empty placeholder targets\n",
    "        targets = [boxes for boxes in targets if boxes is not None]\n",
    "\n",
    "        # Add sample index to targets\n",
    "        for i, boxes in enumerate(targets):\n",
    "            boxes[:, 0] = i  # Assuming the first column is for sample index\n",
    "\n",
    "        # Concatenate targets for the entire batch\n",
    "        targets = torch.cat(targets, 0)\n",
    "\n",
    "\n",
    "\n",
    "        # Resize images to input shape\n",
    "        # Convert images to tensors and resize them\n",
    "        imgs = torch.stack([self.resize(img, (608, 608)) for img in imgs]) # Use the existing 'resize' function and provide a desired size.\n",
    "\n",
    "        # Update batch count\n",
    "        self.batch_count += 1\n",
    "\n",
    "        return imgs, targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "L1rzaCpW2hCX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the root directory, version, and radar channel\n",
    "root_dir = '/data/sets/nuscenes'\n",
    "version = 'v1.0-mini'\n",
    "split = 'train'\n",
    "radar_channel = 'RADAR_FRONT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YxoLUfTT2jj8"
   },
   "outputs": [],
   "source": [
    "def get_corners(x, y, w, l, yaw):\n",
    "    bev_corners = np.zeros((4, 2), dtype=np.float32)\n",
    "\n",
    "    # front left\n",
    "    bev_corners[0, 0] = x - w / 2 * np.cos(yaw) - l / 2 * np.sin(yaw)\n",
    "    bev_corners[0, 1] = y - w / 2 * np.sin(yaw) + l / 2 * np.cos(yaw)\n",
    "\n",
    "    # rear left\n",
    "    bev_corners[1, 0] = x - w / 2 * np.cos(yaw) + l / 2 * np.sin(yaw)\n",
    "    bev_corners[1, 1] = y - w / 2 * np.sin(yaw) - l / 2 * np.cos(yaw)\n",
    "\n",
    "    # rear right\n",
    "    bev_corners[2, 0] = x + w / 2 * np.cos(yaw) + l / 2 * np.sin(yaw)\n",
    "    bev_corners[2, 1] = y + w / 2 * np.sin(yaw) - l / 2 * np.cos(yaw)\n",
    "\n",
    "    # front right\n",
    "    bev_corners[3, 0] = x + w / 2 * np.cos(yaw) - l / 2 * np.sin(yaw)\n",
    "    bev_corners[3, 1] = y + w / 2 * np.sin(yaw) + l / 2 * np.cos(yaw)\n",
    "\n",
    "    return bev_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shapely in /home/synergy-carla/miniconda3/envs/yolo-env/lib/python3.10/site-packages (2.0.6)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /home/synergy-carla/miniconda3/envs/yolo-env/lib/python3.10/site-packages (from shapely) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade shapely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "JcKn3_YD2oUA"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "def convert_format(boxes_array):\n",
    "    \"\"\"\n",
    "    :param array: an array of shape [# bboxs, 4, 2]\n",
    "    :return: a shapely.geometry.Polygon object\n",
    "    \"\"\"\n",
    "    polygons = [Polygon([(box[i, 0], box[i, 1]) for i in range(4)]) for box in boxes_array]\n",
    "    return np.array(polygons)\n",
    "\n",
    "\n",
    "# def convert_format(boxes_array):\n",
    "#     \"\"\"\n",
    "#     Convert an array of bounding boxes to Shapely Polygons.\n",
    "#     :param boxes_array: an array of shape [# boxes, 4, 2]\n",
    "#     :return: a list of shapely.geometry.Polygon objects\n",
    "#     \"\"\"\n",
    "#     polygons = []\n",
    "#     for box in boxes_array:\n",
    "#         if box.shape == (4, 2):\n",
    "#             if np.any(np.isnan(box)) or np.any(np.isinf(box)):\n",
    "#                 print(f\"Invalid coordinates detected: {box}\")\n",
    "#                 continue\n",
    "#             try:\n",
    "#                 polygon = Polygon([(box[i, 0], box[i, 1]) for i in range(4)] + [(box[0, 0], box[0, 1])])\n",
    "#                 if polygon.is_valid:\n",
    "#                     polygons.append(polygon)\n",
    "#                 else:\n",
    "#                     print(f\"Invalid polygon detected: {polygon}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error creating polygon for box {box}: {e}\")\n",
    "#         else:\n",
    "#             print(f\"Box with incorrect shape: {box.shape}\")\n",
    "#     return np.array(polygons)\n",
    "\n",
    "def compute_iou(box, boxes):\n",
    "    \"\"\"Calculates IoU of the given box with the array of the given boxes.\n",
    "    box: a polygon\n",
    "    boxes: a vector of polygons\n",
    "    Note: the areas are passed in rather than calculated here for\n",
    "    efficiency. Calculate once in the caller to avoid duplicate work.\n",
    "    \"\"\"\n",
    "    # Calculate intersection areas\n",
    "    iou = [box.intersection(b).area / (box.union(b).area + 1e-12) for b in boxes]\n",
    "\n",
    "    return np.array(iou, dtype=np.float32)\n",
    "\n",
    "def to_cpu(tensor):\n",
    "    return tensor.detach().cpu()\n",
    "\n",
    "def load_classes(path):\n",
    "    \"\"\"\n",
    "    Loads class labels at 'path'\n",
    "    \"\"\"\n",
    "    fp = open(path, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names\n",
    "\n",
    "def parse_model_config(path):\n",
    "    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n",
    "    file = open(path, 'r')\n",
    "    lines = file.read().split('\\n')\n",
    "    lines = [x for x in lines if x and not x.startswith('#')]\n",
    "    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces\n",
    "    module_defs = []\n",
    "    for line in lines:\n",
    "        if line.startswith('['): # This marks the start of a new block\n",
    "            module_defs.append({})\n",
    "            module_defs[-1]['type'] = line[1:-1].rstrip()\n",
    "            if module_defs[-1]['type'] == 'convolutional':\n",
    "                module_defs[-1]['batch_normalize'] = 0\n",
    "        else:\n",
    "            key, value = line.split(\"=\")\n",
    "            value = value.strip()\n",
    "            module_defs[-1][key.rstrip()] = value.strip()\n",
    "\n",
    "    return module_defs\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "def rescale_boxes(boxes, current_dim, original_shape):\n",
    "    \"\"\" Rescales bounding boxes to the original shape \"\"\"\n",
    "    orig_h, orig_w = original_shape\n",
    "    # The amount of padding that was added\n",
    "    pad_x = max(orig_h - orig_w, 0) * (current_dim / max(original_shape))\n",
    "    pad_y = max(orig_w - orig_h, 0) * (current_dim / max(original_shape))\n",
    "    # Image height and width after padding is removed\n",
    "    unpad_h = current_dim - pad_y\n",
    "    unpad_w = current_dim - pad_x\n",
    "    # Rescale bounding boxes to dimension of original image\n",
    "    boxes[:, 0] = ((boxes[:, 0] - pad_x // 2) / unpad_w) * orig_w\n",
    "    boxes[:, 1] = ((boxes[:, 1] - pad_y // 2) / unpad_h) * orig_h\n",
    "    boxes[:, 2] = ((boxes[:, 2] - pad_x // 2) / unpad_w) * orig_w\n",
    "    boxes[:, 3] = ((boxes[:, 3] - pad_y // 2) / unpad_h) * orig_h\n",
    "\n",
    "    return boxes\n",
    "\n",
    "def ap_per_class(tp, conf, pred_cls, target_cls):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:    True positives (list).\n",
    "        conf:  Objectness value from 0-1 (list).\n",
    "        pred_cls: Predicted object classes (list).\n",
    "        target_cls: True object classes (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by objectness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes = np.unique(target_cls)\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    ap, p, r = [], [], []\n",
    "    for c in tqdm.tqdm(unique_classes, desc=\"Computing AP\"):\n",
    "        i = pred_cls == c\n",
    "        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n",
    "        n_p = i.sum()  # Number of predicted objects\n",
    "\n",
    "        if n_p == 0 and n_gt == 0:\n",
    "            continue\n",
    "        elif n_p == 0 or n_gt == 0:\n",
    "            ap.append(0)\n",
    "            r.append(0)\n",
    "            p.append(0)\n",
    "        else:\n",
    "            # Accumulate FPs and TPs\n",
    "            fpc = (1 - tp[i]).cumsum()\n",
    "            tpc = (tp[i]).cumsum()\n",
    "\n",
    "            # Recall\n",
    "            recall_curve = tpc / (n_gt + 1e-16)\n",
    "            r.append(recall_curve[-1])\n",
    "\n",
    "            # Precision\n",
    "            precision_curve = tpc / (tpc + fpc)\n",
    "            p.append(precision_curve[-1])\n",
    "\n",
    "            # AP from recall-precision curve\n",
    "            ap.append(compute_ap(recall_curve, precision_curve))\n",
    "\n",
    "    # Compute F1 score (harmonic mean of precision and recall)\n",
    "    p, r, ap = np.array(p), np.array(r), np.array(ap)\n",
    "    f1 = 2 * p * r / (p + r + 1e-16)\n",
    "\n",
    "    return p, r, ap, f1, unique_classes.astype(\"int32\")\n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "def get_batch_statistics_rotated_bbox(outputs, targets, iou_threshold):\n",
    "    \"\"\" Compute true positives, predicted scores and predicted labels per sample \"\"\"\n",
    "    batch_metrics = []\n",
    "    for sample_i in range(len(outputs)):\n",
    "\n",
    "        if outputs[sample_i] is None:\n",
    "            continue\n",
    "\n",
    "        output = outputs[sample_i]\n",
    "        pred_boxes = output[:, :6]\n",
    "        pred_scores = output[:, 6]\n",
    "        pred_labels = output[:, -1]\n",
    "\n",
    "        true_positives = np.zeros(pred_boxes.shape[0])\n",
    "\n",
    "        annotations = targets[targets[:, 0] == sample_i][:, 1:]\n",
    "        target_labels = annotations[:, 0] if len(annotations) else []\n",
    "        if len(annotations):\n",
    "            detected_boxes = []\n",
    "            target_boxes = annotations[:, 1:]\n",
    "\n",
    "            for pred_i, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):\n",
    "\n",
    "                # If targets are found break\n",
    "                if len(detected_boxes) == len(annotations):\n",
    "                    break\n",
    "\n",
    "                # Ignore if label is not one of the target labels\n",
    "                if pred_label not in target_labels:\n",
    "                    continue\n",
    "\n",
    "                #iou, box_index = rotated_bbox_iou(pred_box.unsqueeze(0), target_boxes, 1.0, False).squeeze().max(0)\n",
    "                ious = rotated_bbox_iou_polygon(pred_box, target_boxes)\n",
    "                iou, box_index = torch.from_numpy(ious).max(0)\n",
    "\n",
    "                if iou >= iou_threshold and box_index not in detected_boxes:\n",
    "                    true_positives[pred_i] = 1\n",
    "                    detected_boxes += [box_index]\n",
    "        batch_metrics.append([true_positives, pred_scores, pred_labels])\n",
    "    return batch_metrics\n",
    "\n",
    "def rotated_box_wh_iou_polygon(anchor, wh, imre):\n",
    "    w1, h1, im1, re1 = anchor[0], anchor[1], anchor[2], anchor[3]\n",
    "\n",
    "    wh = wh.t()\n",
    "    imre = imre.t()\n",
    "    w2, h2, im2, re2 = wh[0], wh[1], imre[0], imre[1]\n",
    "\n",
    "    anchor_box = torch.FloatTensor([100, 100, w1, h1, im1, re1]).view(-1, 6).to(device) # Use torch.FloatTensor and move to device\n",
    "    target_boxes = torch.FloatTensor(w2.shape[0], 6).fill_(100).to(device)\n",
    "    target_boxes[:, 2] = w2\n",
    "    target_boxes[:, 3] = h2\n",
    "    target_boxes[:, 4] = im2\n",
    "    target_boxes[:, 5] = re2\n",
    "\n",
    "    ious = rotated_bbox_iou_polygon(anchor_box[0], target_boxes)\n",
    "\n",
    "    return torch.from_numpy(ious)\n",
    "\n",
    "def rotated_box_11_iou_polygon(box1, box2, nG):\n",
    "\n",
    "    box1_new = torch.cuda.FloatTensor(box1.shape[0], 6).fill_(0)\n",
    "    box2_new = torch.cuda.FloatTensor(box2.shape[0], 6).fill_(0)\n",
    "\n",
    "    box1_new[:, :4] = box1[:, :4]\n",
    "    box1_new[:, 4:] = box1[:, 4:]\n",
    "\n",
    "    box2_new[:, :4] = box2[:, :4] * nG\n",
    "    box2_new[:, 4:] = box2[:, 4:]\n",
    "\n",
    "    ious = []\n",
    "    for i in range(box1_new.shape[0]):\n",
    "        bbox1 = box1_new[i]\n",
    "        bbox2 = box2_new[i].view(-1, 6)\n",
    "\n",
    "        iou = rotated_bbox_iou_polygon(bbox1, bbox2).squeeze()\n",
    "        ious.append(iou)\n",
    "\n",
    "    ious = np.array(ious)\n",
    "\n",
    "    return torch.from_numpy(ious)\n",
    "\n",
    "def rotated_bbox_iou_polygon(box1, box2):\n",
    "    box1 = to_cpu(box1).numpy()\n",
    "    box2 = to_cpu(box2).numpy()\n",
    "\n",
    "    x,y,w,l,im,re = box1\n",
    "    angle = np.arctan2(im, re)\n",
    "    bbox1 = np.array(get_corners(x, y, w, l, angle)).reshape(-1,4,2)\n",
    "    bbox1 = convert_format(bbox1)\n",
    "\n",
    "    bbox2 = []\n",
    "    for i in range(box2.shape[0]):\n",
    "        x,y,w,l,im,re = box2[i,:]\n",
    "        angle = np.arctan2(im, re)\n",
    "        bev_corners = get_corners(x, y, w, l, angle)\n",
    "        bbox2.append(bev_corners)\n",
    "    bbox2 = convert_format(np.array(bbox2))\n",
    "\n",
    "    return compute_iou(bbox1[0], bbox2)\n",
    "\n",
    "def non_max_suppression_rotated_bbox(prediction, conf_thres=0.95, nms_thres=0.4):\n",
    "    \"\"\"\n",
    "        Removes detections with lower object confidence score than 'conf_thres' and performs\n",
    "        Non-Maximum Suppression to further filter detections.\n",
    "        Returns detections with shape:\n",
    "            (x, y, w, l, im, re, object_conf, class_score, class_pred)\n",
    "    \"\"\"\n",
    "\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for image_i, image_pred in enumerate(prediction):\n",
    "        # Filter out confidence scores below threshold\n",
    "        image_pred = image_pred[image_pred[:, 6] >= conf_thres]\n",
    "        # If none are remaining => process next image\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        # Object confidence times class confidence\n",
    "        score = image_pred[:, 6] * image_pred[:, 7:].max(1)[0]\n",
    "        # Sort by it\n",
    "        image_pred = image_pred[(-score).argsort()]\n",
    "        class_confs, class_preds = image_pred[:, 7:].max(1, keepdim=True)\n",
    "        detections = torch.cat((image_pred[:, :7].float(), class_confs.float(), class_preds.float()), 1)\n",
    "        # Perform non-maximum suppression\n",
    "        keep_boxes = []\n",
    "        while detections.size(0):\n",
    "            #large_overlap = rotated_bbox_iou(detections[0, :6].unsqueeze(0), detections[:, :6], 1.0, False) > nms_thres # not working\n",
    "            large_overlap = rotated_bbox_iou_polygon(detections[0, :6], detections[:, :6]) > nms_thres\n",
    "            # large_overlap = torch.from_numpy(large_overlap.astype('uint8'))\n",
    "            large_overlap = torch.from_numpy(large_overlap)\n",
    "            label_match = detections[0, -1] == detections[:, -1]\n",
    "            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n",
    "            invalid = large_overlap & label_match\n",
    "            weights = detections[invalid, 6:7]\n",
    "            # Merge overlapping bboxes by order of confidence\n",
    "            detections[0, :6] = (weights * detections[invalid, :6]).sum(0) / weights.sum()\n",
    "            keep_boxes += [detections[0]]\n",
    "            detections = detections[~invalid]\n",
    "        if keep_boxes:\n",
    "            output[image_i] = torch.stack(keep_boxes)\n",
    "\n",
    "    return output\n",
    "\n",
    "def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n",
    "    device = pred_boxes.device  # Get the device (e.g., cuda:0) from pred_boxes\n",
    "\n",
    "    ByteTensor = torch.cuda.ByteTensor if pred_boxes.is_cuda else torch.ByteTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n",
    "\n",
    "    nB = pred_boxes.size(0)\n",
    "    nA = pred_boxes.size(1)\n",
    "    nC = pred_cls.size(-1)\n",
    "    nG = pred_boxes.size(2)\n",
    "\n",
    "    # Output tensors on the same device\n",
    "    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0).to(device)\n",
    "    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1).to(device)\n",
    "    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0).to(device)\n",
    "    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0).to(device)\n",
    "    tx = FloatTensor(nB, nA, nG, nG).fill_(0).to(device)\n",
    "    ty = FloatTensor(nB, nA, nG, nG).fill_(0).to(device)\n",
    "    tw = FloatTensor(nB, nA, nG, nG).fill_(0).to(device)\n",
    "    th = FloatTensor(nB, nA, nG, nG).fill_(0).to(device)\n",
    "    tim = FloatTensor(nB, nA, nG, nG).fill_(0).to(device)\n",
    "    tre = FloatTensor(nB, nA, nG, nG).fill_(0).to(device)\n",
    "    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0).to(device)\n",
    "\n",
    "    # Convert to position relative to box\n",
    "    target_boxes = target[:, 2:8].to(device)  # Move to the correct device\n",
    "\n",
    "    gxy = target_boxes[:, :2] * nG\n",
    "    gwh = target_boxes[:, 2:4] * nG\n",
    "    gimre = target_boxes[:, 4:]\n",
    "\n",
    "    # Get anchors with best iou\n",
    "    ious = torch.stack([rotated_box_wh_iou_polygon(anchor.to(device), gwh, gimre) for anchor in anchors])\n",
    "\n",
    "    best_ious, best_n = ious.max(0)\n",
    "    b, target_labels = target[:, :2].long().t()\n",
    "    b = b.to(device)\n",
    "\n",
    "    gx, gy = gxy.t()\n",
    "    gw, gh = gwh.t()\n",
    "    gim, gre = gimre.t()\n",
    "    gi, gj = gxy.long().t()\n",
    "\n",
    "    # Clamp indices to be within bounds\n",
    "    gj = gj.clamp(0, nG - 1)\n",
    "    gi = gi.clamp(0, nG - 1)\n",
    "\n",
    "    # # Debugging information\n",
    "    # print(\"Shape of obj_mask:\", obj_mask.shape)\n",
    "    # print(\"Shape of noobj_mask:\", noobj_mask.shape)\n",
    "    # print(\"Shape of tx:\", tx.shape)\n",
    "    # print(\"Shape of ty:\", ty.shape)\n",
    "    # print(\"Shape of tw:\", tw.shape)\n",
    "    # print(\"Shape of th:\", th.shape)\n",
    "    # print(\"Shape of tim:\", tim.shape)\n",
    "    # print(\"Shape of tre:\", tre.shape)\n",
    "    # print(\"Shape of tcls:\", tcls.shape)\n",
    "    # print(\"Unique indices for b:\", b.unique())\n",
    "    # print(\"Unique indices for best_n:\", best_n.unique())\n",
    "    # print(\"Unique indices for gj:\", gj.unique())\n",
    "    # print(\"Unique indices for gi:\", gi.unique())\n",
    "\n",
    "    # Ensure indices are within bounds\n",
    "    assert (gj >= 0).all() and (gj < nG).all(), \"gj out of bounds\"\n",
    "    assert (gi >= 0).all() and (gi < nG).all(), \"gi out of bounds\"\n",
    "\n",
    "    # Set masks\n",
    "    obj_mask[b, best_n, gj, gi] = 1\n",
    "    noobj_mask[b, best_n, gj, gi] = 0\n",
    "\n",
    "    # Set noobj mask to zero where iou exceeds ignore threshold\n",
    "    for i, anchor_ious in enumerate(ious.t()):\n",
    "        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n",
    "\n",
    "    # Coordinates\n",
    "    tx[b, best_n, gj, gi] = (gx - gx.floor()).type_as(tx)\n",
    "    ty[b, best_n, gj, gi] = (gy - gy.floor()).type_as(ty)\n",
    "    # Width and height\n",
    "    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0].to(device) + 1e-16).type_as(tw)\n",
    "    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1].to(device) + 1e-16).type_as(th)\n",
    "    # Im and real part\n",
    "    tim[b, best_n, gj, gi] = gim\n",
    "    tre[b, best_n, gj, gi] = gre\n",
    "\n",
    "    # One-hot encoding of label\n",
    "    tcls[b, best_n, gj, gi, target_labels] = 1\n",
    "    # Compute label correctness and iou at best anchor\n",
    "    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n",
    "\n",
    "    # Ensure the rotated_iou_scores are on the correct device\n",
    "    rotated_iou_scores = rotated_box_11_iou_polygon(pred_boxes[b, best_n, gj, gi], target_boxes, nG)\n",
    "    iou_scores[b, best_n, gj, gi] = rotated_iou_scores.to(device)\n",
    "\n",
    "    tconf = obj_mask.float()\n",
    "    return iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tim, tre, tcls, tconf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XKq4X4Bw2sKO"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "def create_modules(module_defs):\n",
    "    \"\"\"\n",
    "    Constructs module list of layer blocks from module configuration in module_defs\n",
    "    \"\"\"\n",
    "    hyperparams = module_defs.pop(0)\n",
    "    output_filters = [int(hyperparams[\"channels\"])]\n",
    "    module_list = nn.ModuleList()\n",
    "    for module_i, module_def in enumerate(module_defs):\n",
    "        modules = nn.Sequential()\n",
    "\n",
    "        if module_def[\"type\"] == \"convolutional\":\n",
    "            bn = int(module_def[\"batch_normalize\"])\n",
    "            filters = int(module_def[\"filters\"])\n",
    "            kernel_size = int(module_def[\"size\"])\n",
    "            pad = (kernel_size - 1) // 2\n",
    "            modules.add_module(\n",
    "                f\"conv_{module_i}\",\n",
    "                nn.Conv2d(\n",
    "                    in_channels=output_filters[-1],\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=int(module_def[\"stride\"]),\n",
    "                    padding=pad,\n",
    "                    bias=not bn,\n",
    "                ),\n",
    "            )\n",
    "            if bn:\n",
    "                modules.add_module(f\"batch_norm_{module_i}\", nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5))\n",
    "            if module_def[\"activation\"] == \"leaky\":\n",
    "                modules.add_module(f\"leaky_{module_i}\", nn.LeakyReLU(0.1))\n",
    "\n",
    "        elif module_def[\"type\"] == \"maxpool\":\n",
    "            kernel_size = int(module_def[\"size\"])\n",
    "            stride = int(module_def[\"stride\"])\n",
    "            if kernel_size == 2 and stride == 1:\n",
    "                modules.add_module(f\"_debug_padding_{module_i}\", nn.ZeroPad2d((0, 1, 0, 1)))\n",
    "            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - 1) // 2))\n",
    "            modules.add_module(f\"maxpool_{module_i}\", maxpool)\n",
    "\n",
    "        elif module_def[\"type\"] == \"upsample\":\n",
    "            upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n",
    "            modules.add_module(f\"upsample_{module_i}\", upsample)\n",
    "\n",
    "        elif module_def[\"type\"] == \"route\":\n",
    "            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n",
    "            filters = sum([output_filters[1:][i] for i in layers])\n",
    "            modules.add_module(f\"route_{module_i}\", EmptyLayer())\n",
    "\n",
    "        elif module_def[\"type\"] == \"shortcut\":\n",
    "            filters = output_filters[1:][int(module_def[\"from\"])]\n",
    "            modules.add_module(f\"shortcut_{module_i}\", EmptyLayer())\n",
    "\n",
    "        elif module_def[\"type\"] == \"yolo\":\n",
    "            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n",
    "            # Extract anchors\n",
    "            anchors = [float(x) for x in module_def[\"anchors\"].split(\",\")]\n",
    "            anchors = [(anchors[i], anchors[i + 1], math.sin(anchors[i + 2]), math.cos(anchors[i + 2])) for i in range(0, len(anchors), 3)]\n",
    "            anchors = [anchors[i] for i in anchor_idxs]\n",
    "            num_classes = int(module_def[\"classes\"])\n",
    "            img_size = int(hyperparams[\"height\"])\n",
    "            # Define detection layer\n",
    "            yolo_layer = YOLOLayer(anchors, num_classes, img_size)\n",
    "            modules.add_module(f\"yolo_{module_i}\", yolo_layer)\n",
    "        # Register module list and number of output filters\n",
    "        module_list.append(modules)\n",
    "        output_filters.append(filters)\n",
    "\n",
    "    return hyperparams, module_list\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\" nn.Upsample is deprecated \"\"\"\n",
    "\n",
    "    def __init__(self, scale_factor, mode=\"nearest\"):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmptyLayer(nn.Module):\n",
    "    \"\"\"Placeholder for 'route' and 'shortcut' layers\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(EmptyLayer, self).__init__()\n",
    "\n",
    "\n",
    "class YOLOLayer(nn.Module):\n",
    "    \"\"\"Detection layer\"\"\"\n",
    "\n",
    "    def __init__(self, anchors, num_classes, img_dim=416):\n",
    "        super(YOLOLayer, self).__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_thres = 0.5\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.obj_scale = 1\n",
    "        self.noobj_scale = 100\n",
    "        self.metrics = {}\n",
    "        self.img_dim = img_dim\n",
    "        self.grid_size = 0  # grid size\n",
    "\n",
    "    def compute_grid_offsets(self, grid_size, cuda=True):\n",
    "        self.grid_size = grid_size\n",
    "        g = self.grid_size\n",
    "        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "        self.stride = self.img_dim / self.grid_size\n",
    "        # Calculate offsets for each grid\n",
    "        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n",
    "        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n",
    "        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride, im, re) for a_w, a_h, im, re in self.anchors])\n",
    "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
    "        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
    "\n",
    "    def forward(self, x, targets=None, img_dim=None):\n",
    "\n",
    "        # Tensors for cuda support\n",
    "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
    "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
    "        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n",
    "\n",
    "        self.img_dim = img_dim\n",
    "        num_samples = x.size(0)\n",
    "        grid_size = x.size(2)\n",
    "\n",
    "        prediction = (\n",
    "            x.view(num_samples, self.num_anchors, self.num_classes + 7, grid_size, grid_size)\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "            .contiguous()\n",
    "        )\n",
    "\n",
    "        # Get outputs\n",
    "        x = torch.sigmoid(prediction[..., 0])  # Center x\n",
    "        y = torch.sigmoid(prediction[..., 1])  # Center y\n",
    "        w = prediction[..., 2]  # Width\n",
    "        h = prediction[..., 3]  # Height\n",
    "        im = prediction[..., 4]  # angle imaginary part\n",
    "        re = prediction[..., 5]  # angle real part\n",
    "        pred_conf = torch.sigmoid(prediction[..., 6])  # Conf\n",
    "        pred_cls = torch.sigmoid(prediction[..., 7:])  # Cls pred.\n",
    "\n",
    "        # If grid size does not match current we compute new offsets\n",
    "        if grid_size != self.grid_size:\n",
    "            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)\n",
    "\n",
    "        # Add offset and scale with anchors\n",
    "        pred_boxes = FloatTensor(prediction[..., :6].shape)\n",
    "        pred_boxes[..., 0] = x.data + self.grid_x\n",
    "        pred_boxes[..., 1] = y.data + self.grid_y\n",
    "        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w\n",
    "        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h\n",
    "        pred_boxes[..., 4] = im\n",
    "        pred_boxes[..., 5] = re\n",
    "\n",
    "        output = torch.cat(\n",
    "            (\n",
    "                #pred_boxes.view(num_samples, -1, 6) * self.stride,\n",
    "                pred_boxes[..., :4].view(num_samples, -1, 4) * self.stride,\n",
    "                pred_boxes[..., 4:].view(num_samples, -1, 2),\n",
    "                pred_conf.view(num_samples, -1, 1),\n",
    "                pred_cls.view(num_samples, -1, self.num_classes),\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "        if targets is None:\n",
    "            return output, 0\n",
    "        else:\n",
    "            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tim, tre, tcls, tconf = build_targets(\n",
    "                pred_boxes=pred_boxes,\n",
    "                pred_cls=pred_cls,\n",
    "                target=targets,\n",
    "                anchors=self.scaled_anchors,\n",
    "                ignore_thres=self.ignore_thres,\n",
    "            )\n",
    "\n",
    "            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n",
    "            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n",
    "            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n",
    "            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n",
    "            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n",
    "            loss_im = self.mse_loss(im[obj_mask], tim[obj_mask])\n",
    "            loss_re = self.mse_loss(re[obj_mask], tre[obj_mask])\n",
    "            loss_eular = loss_im + loss_re\n",
    "            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n",
    "            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n",
    "            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n",
    "            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n",
    "            total_loss = loss_x + loss_y + loss_w + loss_h + loss_eular + loss_conf + loss_cls\n",
    "\n",
    "            # Metrics\n",
    "            cls_acc = 100 * class_mask[obj_mask].mean()\n",
    "            conf_obj = pred_conf[obj_mask].mean()\n",
    "            conf_noobj = pred_conf[noobj_mask].mean()\n",
    "            conf50 = (pred_conf > 0.5).float()\n",
    "            iou50 = (iou_scores > 0.5).float()\n",
    "            iou75 = (iou_scores > 0.75).float()\n",
    "            detected_mask = conf50 * class_mask * tconf\n",
    "            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + 1e-16)\n",
    "            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + 1e-16)\n",
    "            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + 1e-16)\n",
    "\n",
    "            self.metrics = {\n",
    "                \"loss\": to_cpu(total_loss).item(),\n",
    "                \"x\": to_cpu(loss_x).item(),\n",
    "                \"y\": to_cpu(loss_y).item(),\n",
    "                \"w\": to_cpu(loss_w).item(),\n",
    "                \"h\": to_cpu(loss_h).item(),\n",
    "                \"im\": to_cpu(loss_im).item(),\n",
    "                \"re\": to_cpu(loss_re).item(),\n",
    "                \"conf\": to_cpu(loss_conf).item(),\n",
    "                \"cls\": to_cpu(loss_cls).item(),\n",
    "                \"cls_acc\": to_cpu(cls_acc).item(),\n",
    "                \"recall50\": to_cpu(recall50).item(),\n",
    "                \"recall75\": to_cpu(recall75).item(),\n",
    "                \"precision\": to_cpu(precision).item(),\n",
    "                \"conf_obj\": to_cpu(conf_obj).item(),\n",
    "                \"conf_noobj\": to_cpu(conf_noobj).item(),\n",
    "                \"grid_size\": grid_size,\n",
    "            }\n",
    "\n",
    "            return output, total_loss\n",
    "\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    \"\"\"YOLOv3 object detection model\"\"\"\n",
    "\n",
    "    def __init__(self, config_path, img_size=416):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.module_defs = parse_model_config(config_path)\n",
    "        self.hyperparams, self.module_list = create_modules(self.module_defs)\n",
    "        self.yolo_layers = [layer[0] for layer in self.module_list if hasattr(layer[0], \"metrics\")]\n",
    "        self.img_size = img_size\n",
    "        self.seen = 0\n",
    "        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        img_dim = x.shape[2]\n",
    "        loss = 0\n",
    "        layer_outputs, yolo_outputs = [], []\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
    "                x = module(x)\n",
    "            elif module_def[\"type\"] == \"route\":\n",
    "                x = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[\"layers\"].split(\",\")], 1)\n",
    "            elif module_def[\"type\"] == \"shortcut\":\n",
    "                layer_i = int(module_def[\"from\"])\n",
    "                x = layer_outputs[-1] + layer_outputs[layer_i]\n",
    "            elif module_def[\"type\"] == \"yolo\":\n",
    "                x, layer_loss = module[0](x, targets, img_dim)\n",
    "                loss += layer_loss\n",
    "                yolo_outputs.append(x)\n",
    "            layer_outputs.append(x)\n",
    "        yolo_outputs = to_cpu(torch.cat(yolo_outputs, 1))\n",
    "        return yolo_outputs if targets is None else (loss, yolo_outputs)\n",
    "\n",
    "    def load_darknet_weights(self, weights_path):\n",
    "        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n",
    "\n",
    "        # Open the weights file\n",
    "        with open(weights_path, \"rb\") as f:\n",
    "            header = np.fromfile(f, dtype=np.int32, count=5)  # First five are header values\n",
    "            self.header_info = header  # Needed to write header when saving weights\n",
    "            self.seen = header[3]  # number of images seen during training\n",
    "            weights = np.fromfile(f, dtype=np.float32)  # The rest are weights\n",
    "\n",
    "        # Establish cutoff for loading backbone weights\n",
    "        cutoff = None\n",
    "        if \"darknet53.conv.74\" in weights_path:\n",
    "            cutoff = 75\n",
    "        elif \"yolov3-tiny.conv.15\" in weights_path:\n",
    "            cutoff = 15\n",
    "\n",
    "        ptr = 0\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if i == cutoff:\n",
    "                break\n",
    "            if module_def[\"type\"] == \"convolutional\":\n",
    "                conv_layer = module[0]\n",
    "                if module_def[\"batch_normalize\"]:\n",
    "                    # Load BN bias, weights, running mean and running variance\n",
    "                    bn_layer = module[1]\n",
    "                    num_b = bn_layer.bias.numel()  # Number of biases\n",
    "                    # Bias\n",
    "                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)\n",
    "                    bn_layer.bias.data.copy_(bn_b)\n",
    "                    ptr += num_b\n",
    "                    # Weight\n",
    "                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)\n",
    "                    bn_layer.weight.data.copy_(bn_w)\n",
    "                    ptr += num_b\n",
    "                    # Running Mean\n",
    "                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)\n",
    "                    bn_layer.running_mean.data.copy_(bn_rm)\n",
    "                    ptr += num_b\n",
    "                    # Running Var\n",
    "                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)\n",
    "                    bn_layer.running_var.data.copy_(bn_rv)\n",
    "                    ptr += num_b\n",
    "                else:\n",
    "                    # Load conv. bias\n",
    "                    num_b = conv_layer.bias.numel()\n",
    "                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)\n",
    "                    conv_layer.bias.data.copy_(conv_b)\n",
    "                    ptr += num_b\n",
    "                # Load conv. weights\n",
    "                num_w = conv_layer.weight.numel()\n",
    "                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)\n",
    "                conv_layer.weight.data.copy_(conv_w)\n",
    "                ptr += num_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zosWoT_v2uvW"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "\n",
    "\n",
    "import os, sys, time, datetime, argparse\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, iou_thres, conf_thres, nms_thres, img_size, batch_size):\n",
    "    model.eval()\n",
    "\n",
    "    # Get dataloader\n",
    "    split='val'\n",
    "    dataset = NuScenesRadarDataset(root_dir=root_dir, version=version, set=split, radar_channel=radar_channel)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False, num_workers=1, collate_fn=dataset.collate_fn\n",
    "    )\n",
    "\n",
    "    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "    labels = []\n",
    "    sample_metrics = []  # List of tuples (TP, confs, pred)\n",
    "    for batch_i, (imgs, targets) in enumerate(tqdm.tqdm(dataloader, desc=\"Detecting objects\")):\n",
    "\n",
    "        # Extract labels\n",
    "        labels += targets[:, 1].tolist()\n",
    "        # Rescale target\n",
    "        targets[:, 2:] *= img_size\n",
    "\n",
    "        imgs = Variable(imgs.type(Tensor), requires_grad=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(imgs)\n",
    "            outputs = non_max_suppression_rotated_bbox(outputs, conf_thres=conf_thres, nms_thres=nms_thres)\n",
    "\n",
    "        sample_metrics += get_batch_statistics_rotated_bbox(outputs, targets, iou_threshold=iou_thres)\n",
    "\n",
    "    # Concatenate sample statistics\n",
    "    true_positives, pred_scores, pred_labels = [np.concatenate(x, 0) for x in list(zip(*sample_metrics))]\n",
    "    precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, labels)\n",
    "\n",
    "    return precision, recall, AP, f1, ap_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "AHpZnmFO2xeu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 13:01:51.934362: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-28 13:01:52.454731: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-28 13:01:52.592072: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-28 13:01:53.543636: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-28 13:01:58.863232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.compat.v1.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        summary = tf.compat.v1.Summary(value=[tf.compat.v1.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def list_of_scalars_summary(self, tag_value_pairs, step):\n",
    "        \"\"\"Log scalar variables.\"\"\"\n",
    "        summary = tf.compat.v1.Summary(value=[tf.compat.v1.Summary.Value(tag=tag, simple_value=value) for tag, value in tag_value_pairs])\n",
    "        self.writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ERC2Rajp20Es"
   },
   "outputs": [],
   "source": [
    "cfg_content =\"\"\"\n",
    "\n",
    "[net]\n",
    "# Testing\n",
    "# batch=1\n",
    "# subdivisions=1\n",
    "# Training\n",
    "batch=64\n",
    "subdivisions=16\n",
    "width=608\n",
    "height=608\n",
    "channels=3\n",
    "momentum=0.9\n",
    "decay=0.0005\n",
    "angle=0\n",
    "saturation = 1.5\n",
    "exposure = 1.5\n",
    "hue=.1\n",
    "\n",
    "learning_rate=0.001\n",
    "burn_in=1000\n",
    "max_batches = 500200\n",
    "policy=steps\n",
    "steps=400000,450000\n",
    "scales=.1,.1\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=32\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "# Downsample\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=32\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "# Downsample\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "# Downsample\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "# Downsample\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "# Downsample\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=1024\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=1024\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=1024\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=1024\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=1024\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "######################\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=1024\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=1024\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=1024\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "filters=30\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[yolo]\n",
    "mask = 6,7,8\n",
    "#anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "anchors = 11,14,-3.14,  11,14,0,  11,14,3.14,  11,25,-3.14,  11,25,0,  11,25,3.14,  23,51,-3.14,  23,51,0,  23,51,3.14\n",
    "\n",
    "classes=3\n",
    "num=9\n",
    "jitter=.3\n",
    "ignore_thresh = .7\n",
    "truth_thresh = 1\n",
    "random=1\n",
    "\n",
    "\n",
    "[route]\n",
    "layers = -4\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[upsample]\n",
    "stride=2\n",
    "\n",
    "[route]\n",
    "layers = -1, 61\n",
    "\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=512\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=512\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=512\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "filters=30\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[yolo]\n",
    "mask = 3,4,5\n",
    "#anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "anchors = 11,14,-3.14,  11,14,0,  11,14,3.14,  11,25,-3.14,  11,25,0,  11,25,3.14,  23,51,-3.14,  23,51,0,  23,51,3.14\n",
    "\n",
    "\n",
    "\n",
    "classes=3\n",
    "num=9\n",
    "jitter=.3\n",
    "ignore_thresh = .7\n",
    "truth_thresh = 1\n",
    "random=1\n",
    "\n",
    "\n",
    "\n",
    "[route]\n",
    "layers = -4\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[upsample]\n",
    "stride=2\n",
    "\n",
    "[route]\n",
    "layers = -1, 36\n",
    "\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=256\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=256\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=256\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "filters=30\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[yolo]\n",
    "mask = 0,1,2\n",
    "#anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "anchors = 11,14,-3.14,  11,14,0,  11,14,3.14,  11,25,-3.14,  11,25,0,  11,25,3.14,  23,51,-3.14,  23,51,0,  23,51,3.14\n",
    "\n",
    "\n",
    "\n",
    "classes=3\n",
    "num=9\n",
    "jitter=.3\n",
    "ignore_thresh = .7\n",
    "truth_thresh = 1\n",
    "random=1\"\"\"\n",
    "with open(\"complex_yolov3.cfg\", \"w\") as cfg_file:\n",
    "    cfg_file.write(cfg_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOw_eDwM228H",
    "outputId": "bac78199-0970-4bf9-f9b3-adec0c31b5ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created at: ./content/classes_names.txt\n",
      "File content:\n",
      "pedestrian\n",
      "vehicle\n",
      "animal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the file\n",
    "file_path = './content/classes_names.txt'\n",
    "\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write('pedestrian\\n')\n",
    "    file.write('vehicle\\n')\n",
    "    file.write('animal\\n')\n",
    "\n",
    "print(f\"File created at: {file_path}\")\n",
    "\n",
    "# Read and display the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(\"File content:\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jccQGxBb25gi",
    "outputId": "4a530084-e974-41f6-c7d1-c8608ac7f5a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'checkpoints' already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder name\n",
    "folder_name = 'checkpoints'\n",
    "\n",
    "# Create the folder if it does not exist\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(f\"Folder '{folder_name}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46Kga-3528nS",
    "outputId": "a68ad071-ef3d-43de-9dbd-71eb4a62923f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: terminaltables in /home/synergy-carla/miniconda3/envs/yolo-env/lib/python3.10/site-packages (3.1.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install terminaltables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oG1IP4z2_LL",
    "outputId": "52eb364f-95b7-4fe0-bbc9-73f8b985e845"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import datetime\n",
    "# import tensorflow as tf\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.autograd import Variable\n",
    "# import torch.optim as optim\n",
    "# from IPython.display import clear_output\n",
    "# # from terminaltables import AsciiTable\n",
    "# import gc\n",
    "# from shapely.geometry import Polygon\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the Logger class using TensorFlow v2 SummaryWriter\n",
    "# class Logger(object):\n",
    "#     def __init__(self, log_dir):\n",
    "#         \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "#         self.writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "#     def scalar_summary(self, tag, value, step):\n",
    "#         \"\"\"Log a scalar variable.\"\"\"\n",
    "#         with self.writer.as_default():\n",
    "#             tf.summary.scalar(tag, value, step=step)\n",
    "#             self.writer.flush()\n",
    "\n",
    "#     def list_of_scalars_summary(self, tag_value_pairs, step):\n",
    "#         \"\"\"Log scalar variables.\"\"\"\n",
    "#         with self.writer.as_default():\n",
    "#             for tag, value in tag_value_pairs:\n",
    "#                 tf.summary.scalar(tag, value, step=step)\n",
    "#             self.writer.flush()\n",
    "\n",
    "# def clear_gpu_memory():\n",
    "#     print(\"Cleared GPU\")\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "# def validate_boxes(boxes_array):\n",
    "#     \"\"\"Filter out invalid or empty boxes.\"\"\"\n",
    "#     valid_boxes = []\n",
    "#     for box in boxes_array:\n",
    "#         if box.shape == (4, 2) and not np.any(np.isnan(box)) and not np.any(np.isinf(box)):\n",
    "#             valid_boxes.append(box)\n",
    "#         else:\n",
    "#             print(f\"Invalid box detected: {box}\")\n",
    "#     return np.array(valid_boxes)\n",
    "\n",
    "# def convert_format(boxes_array):\n",
    "#     \"\"\"\n",
    "#     Convert an array of bounding boxes to Shapely Polygons.\n",
    "#     :param boxes_array: an array of shape [# boxes, 4, 2]\n",
    "#     :return: a list of shapely.geometry.Polygon objects\n",
    "#     \"\"\"\n",
    "#     polygons = []\n",
    "#     for box in boxes_array:\n",
    "#         if box.shape == (4, 2):\n",
    "#             try:\n",
    "#                 polygons.append(Polygon([(box[i, 0], box[i, 1]) for i in range(4)]))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error creating polygon for box {box}: {e}\")\n",
    "#         else:\n",
    "#             print(f\"Box with incorrect shape: {box.shape}\")\n",
    "#     return np.array(polygons)\n",
    "\n",
    "# # Set configuration variables\n",
    "# epochs = 300\n",
    "# batch_size = 1\n",
    "# gradient_accumulations = 2\n",
    "# model_def = \"./content/complex_yolov3.cfg\"\n",
    "# img_size = 608  # BEV_WIDTH\n",
    "# evaluation_interval = 2\n",
    "# multiscale_training = True\n",
    "\n",
    "# # Initialize logger using TensorFlow v2 SummaryWriter\n",
    "# log_dir = \"logs\"\n",
    "# logger = Logger(log_dir)  # Pass the log_dir to your Logger class\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "# class_names = load_classes(\"./content/classes_names.txt\")\n",
    "\n",
    "# # Initialize model\n",
    "# model = Darknet(model_def, img_size=img_size).to(device)\n",
    "# model.apply(weights_init_normal)  # Initialize weights\n",
    "\n",
    "# # Initialize dataset\n",
    "# cwd = os.getcwd()\n",
    "# root_dir = cwd+'/data/sets/nuscenes'\n",
    "# version = 'v1.0-mini'\n",
    "# split = 'train'\n",
    "# radar_channel = 'RADAR_FRONT'\n",
    "\n",
    "# dataset = NuScenesRadarDataset(root_dir=root_dir, version=version, set=split, radar_channel=radar_channel)\n",
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=dataset.collate_fn\n",
    "# )\n",
    "\n",
    "# # Evaluation dataset\n",
    "# eval_root_dir = cwd+'/data/sets/nuscenes'\n",
    "# eval_version = 'v1.0-mini'\n",
    "# eval_split = 'val'\n",
    "\n",
    "# eval_dataset = NuScenesRadarDataset(\n",
    "#     root_dir=eval_root_dir,\n",
    "#     version=eval_version,\n",
    "#     set=eval_split,\n",
    "#     radar_channel=radar_channel\n",
    "# )\n",
    "# eval_dataloader = DataLoader(\n",
    "#     eval_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=eval_dataset.collate_fn\n",
    "# )\n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# metrics = [\n",
    "#     \"grid_size\",\n",
    "#     \"loss\",\n",
    "#     \"x\",\n",
    "#     \"y\",\n",
    "#     \"w\",\n",
    "#     \"h\",\n",
    "#     \"im\",\n",
    "#     \"re\",\n",
    "#     \"conf\",\n",
    "#     \"cls\",\n",
    "#     \"cls_acc\",\n",
    "#     \"recall50\",\n",
    "#     \"recall75\",\n",
    "#     \"precision\",\n",
    "#     \"conf_obj\",\n",
    "#     \"conf_noobj\",\n",
    "# ]\n",
    "# save_path = cwd+\"/content/checkpoints\"\n",
    "# max_mAP = 0.0\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     for batch_i, (imgs, targets) in enumerate(dataloader):\n",
    "#         batches_done = len(dataloader) * epoch + batch_i\n",
    "\n",
    "#         imgs = Variable(imgs.to(device).float())\n",
    "#         targets = Variable(targets.to(device), requires_grad=False)\n",
    "\n",
    "#         if len(targets) == 0:\n",
    "#             continue\n",
    "\n",
    "#         try:\n",
    "#             loss, outputs = model(imgs, targets)\n",
    "#             loss.backward()\n",
    "\n",
    "#             if batches_done % gradient_accumulations == 0:\n",
    "#                 optimizer.step()\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error during training batch {batch_i}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         clear_output(wait=True)\n",
    "#         # Log progress\n",
    "#         log_str = \"\\n---- [Epoch %d/%d, Batch %d/%d] ----\\n\" % (epoch, epochs, batch_i, len(dataloader))\n",
    "\n",
    "#         metric_table = [[\"Metrics\", *[f\"YOLO Layer {i}\" for i in range(len(model.yolo_layers))]]]\n",
    "\n",
    "#         # Log metrics at each YOLO layer\n",
    "#         for i, metric in enumerate(metrics):\n",
    "#             formats = {m: \"%.6f\" for m in metrics}\n",
    "#             formats[\"grid_size\"] = \"%2d\"\n",
    "#             formats[\"cls_acc\"] = \"%.2f%%\"\n",
    "#             row_metrics = [formats[metric] % yolo.metrics.get(metric, 0) for yolo in model.yolo_layers]\n",
    "#             metric_table += [[metric, *row_metrics]]\n",
    "\n",
    "#             # TensorBoard logging\n",
    "#             tensorboard_log = []\n",
    "#             for j, yolo in enumerate(model.yolo_layers):\n",
    "#                 for name, metric in yolo.metrics.items():\n",
    "#                     if name != \"grid_size\":\n",
    "#                         tensorboard_log += [(f\"{name}_{j+1}\", metric)]\n",
    "#             tensorboard_log += [(\"loss\", loss.item())]\n",
    "#             logger.list_of_scalars_summary(tensorboard_log, batches_done)\n",
    "\n",
    "#         log_str += AsciiTable(metric_table).table\n",
    "#         log_str += f\"\\nTotal loss {loss.item()}\"\n",
    "\n",
    "#         # Determine approximate time left for epoch\n",
    "#         epoch_batches_left = len(dataloader) - (batch_i + 1)\n",
    "#         time_left = datetime.timedelta(seconds=epoch_batches_left * (time.time() - start_time) / (batch_i + 1))\n",
    "#         log_str += f\"\\n---- ETA {time_left}\"\n",
    "\n",
    "#         print(log_str)\n",
    "\n",
    "#         model.seen += imgs.size(0)\n",
    "\n",
    "#     if epoch % evaluation_interval == 0:\n",
    "#         print(\"\\n---- Evaluating Model ----\")\n",
    "#         try:\n",
    "#             precision, recall, AP, f1, ap_class = evaluate(\n",
    "#                 model,\n",
    "#                 iou_thres=0.4,\n",
    "#                 conf_thres=0.05,\n",
    "#                 nms_thres=0.04,\n",
    "#                 img_size=img_size,\n",
    "#                 batch_size=8,\n",
    "#             )\n",
    "            \n",
    "#             evaluation_metrics = [\n",
    "#                 (\"val_precision\", precision.mean()),\n",
    "#                 (\"val_recall\", recall.mean()),\n",
    "#                 (\"val_mAP\", AP.mean()),\n",
    "#                 (\"val_f1\", f1.mean()),\n",
    "#             ]\n",
    "#             logger.list_of_scalars_summary(evaluation_metrics, epoch)\n",
    "\n",
    "#             # Print class APs and mAP\n",
    "#             ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
    "#             for i, c in enumerate(ap_class):\n",
    "#                 ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
    "#             print(AsciiTable(ap_table).table)\n",
    "#             print(f\"---- mAP {AP.mean()}\")\n",
    "\n",
    "#             # Save model if mAP is better\n",
    "           \n",
    "#             model_save_path = os.path.join(save_path, f\"yolov3_model_epoch-{epoch}_MAP-{AP.mean():.2f}.pth\")\n",
    "#             torch.save(model.state_dict(), model_save_path)\n",
    "#             max_mAP = AP.mean()\n",
    "#             print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error during evaluation at epoch {epoch}: {e}\")\n",
    "        \n",
    "#         clear_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4VxiOFm3Kkn",
    "outputId": "57f729aa-424f-4195-974a-1bc7af098801",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from shapely.geometry import Polygon\n",
    "from IPython.display import clear_output\n",
    "from terminaltables import AsciiTable\n",
    "import gc\n",
    "\n",
    "# Define the Logger class using TensorFlow v2 SummaryWriter\n",
    "class Logger(object):\n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar(tag, value, step=step)\n",
    "            self.writer.flush()\n",
    "\n",
    "    def list_of_scalars_summary(self, tag_value_pairs, step):\n",
    "        \"\"\"Log scalar variables.\"\"\"\n",
    "        with self.writer.as_default():\n",
    "            for tag, value in tag_value_pairs:\n",
    "                tf.summary.scalar(tag, value, step=step)\n",
    "            self.writer.flush()\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    print(\"Cleared GPU\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# Set configuration variables\n",
    "epochs = 300\n",
    "batch_size = 1\n",
    "gradient_accumulations = 2\n",
    "model_def = \"./content/complex_yolov3.cfg\"\n",
    "img_size = 608  # BEV_WIDTH\n",
    "evaluation_interval = 2\n",
    "\n",
    "# Initialize logger using TensorFlow v2 SummaryWriter\n",
    "log_dir = \"logs\"\n",
    "logger = Logger(log_dir)  # Pass the log_dir to your Logger class\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "class_names = load_classes(\"./content/classes_names.txt\")\n",
    "\n",
    "# Initialize model\n",
    "model = Darknet(model_def, img_size=img_size).to(device)\n",
    "model.apply(weights_init_normal)  # Initialize weights\n",
    "\n",
    "# Initialize dataset\n",
    "cwd = os.getcwd()\n",
    "root_dir = cwd+'/data/sets/nuscenes'\n",
    "version = 'v1.0-mini'\n",
    "split = 'train'\n",
    "radar_channel = 'RADAR_FRONT'\n",
    "\n",
    "dataset = NuScenesRadarDataset(root_dir=root_dir, version=version, set=split, radar_channel=radar_channel)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=dataset.collate_fn\n",
    ")\n",
    "\n",
    "# Evaluation dataset\n",
    "eval_root_dir = cwd+'/data/sets/nuscenes'\n",
    "eval_version = 'v1.0-mini'\n",
    "eval_split = 'val'\n",
    "\n",
    "eval_dataset = NuScenesRadarDataset(\n",
    "    root_dir=eval_root_dir,\n",
    "    version=eval_version,\n",
    "    set=eval_split,\n",
    "    radar_channel=radar_channel\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=eval_dataset.collate_fn\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "metrics = [\n",
    "    \"grid_size\",\n",
    "    \"loss\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"w\",\n",
    "    \"h\",\n",
    "    \"im\",\n",
    "    \"re\",\n",
    "    \"conf\",\n",
    "    \"cls\",\n",
    "    \"cls_acc\",\n",
    "    \"recall50\",\n",
    "    \"recall75\",\n",
    "    \"precision\",\n",
    "    \"conf_obj\",\n",
    "    \"conf_noobj\",\n",
    "]\n",
    "save_path = cwd+\"/content/checkpoints\"\n",
    "max_mAP = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_i, (imgs, targets) in enumerate(dataloader):\n",
    "        batches_done = len(dataloader) * epoch + batch_i\n",
    "\n",
    "        try:\n",
    "            imgs = Variable(imgs.to(device).float())\n",
    "            targets = Variable(targets.to(device), requires_grad=False)\n",
    "\n",
    "            if len(targets) == 0 :\n",
    "                continue  # Skip this batch\n",
    "\n",
    "            loss, outputs = model(imgs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            if batches_done % gradient_accumulations == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Log progress\n",
    "            log_str = \"\\n---- [Epoch %d/%d, Batch %d/%d] ----\\n\" % (epoch, epochs, batch_i, len(dataloader))\n",
    "\n",
    "            metric_table = [[\"Metrics\", *[f\"YOLO Layer {i}\" for i in range(len(model.yolo_layers))]]]\n",
    "\n",
    "            # Log metrics at each YOLO layer\n",
    "            for i, metric in enumerate(metrics):\n",
    "                formats = {m: \"%.6f\" for m in metrics}\n",
    "                formats[\"grid_size\"] = \"%2d\"\n",
    "                formats[\"cls_acc\"] = \"%.2f%%\"\n",
    "                row_metrics = [formats[metric] % yolo.metrics.get(metric, 0) for yolo in model.yolo_layers]\n",
    "                metric_table += [[metric, *row_metrics]]\n",
    "\n",
    "                # TensorBoard logging\n",
    "                tensorboard_log = []\n",
    "                for j, yolo in enumerate(model.yolo_layers):\n",
    "                    for name, metric in yolo.metrics.items():\n",
    "                        if name != \"grid_size\":\n",
    "                            tensorboard_log += [(f\"{name}_{j+1}\", metric)]\n",
    "                tensorboard_log += [(\"loss\", loss.item())]\n",
    "                logger.list_of_scalars_summary(tensorboard_log, batches_done)\n",
    "\n",
    "            log_str += AsciiTable(metric_table).table\n",
    "            log_str += f\"\\nTotal loss {loss.item()}\"\n",
    "\n",
    "            # Determine approximate time left for epoch\n",
    "            epoch_batches_left = len(dataloader) - (batch_i + 1)\n",
    "            time_left = datetime.timedelta(seconds=epoch_batches_left * (time.time() - start_time) / (batch_i + 1))\n",
    "            log_str += f\"\\n---- ETA {time_left}\"\n",
    "\n",
    "            print(log_str)\n",
    "\n",
    "            model.seen += imgs.size(0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training batch {batch_i}: {e}\")\n",
    "            continue  # Skip this batch and move to the next one\n",
    "\n",
    "    if epoch % evaluation_interval == 0:\n",
    "        print(\"\\n---- Evaluating Model ----\")\n",
    "        try:\n",
    "            precision, recall, AP, f1, ap_class = evaluate(\n",
    "                model,\n",
    "                iou_thres=0.4,\n",
    "                conf_thres=0.05,\n",
    "                nms_thres=0.04,\n",
    "                img_size=img_size,\n",
    "                batch_size=8,\n",
    "            )\n",
    "            evaluation_metrics = [\n",
    "                (\"val_precision\", precision.mean()),\n",
    "                (\"val_recall\", recall.mean()),\n",
    "                (\"val_mAP\", AP.mean()),\n",
    "                (\"val_f1\", f1.mean()),\n",
    "            ]\n",
    "            logger.list_of_scalars_summary(evaluation_metrics, epoch)\n",
    "\n",
    "            # Print class APs and mAP\n",
    "            ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
    "            for i, c in enumerate(ap_class):\n",
    "                ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
    "            print(AsciiTable(ap_table).table)\n",
    "            print(f\"---- mAP {AP.mean()}\")\n",
    "\n",
    "            # Save model if mAP is the best so far\n",
    "            \n",
    "            model_save_path = os.path.join(save_path, f\"yolov3_model_epoch-{epoch}_MAP-{AP.mean():.2f}.pth\")\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            max_mAP = AP.mean()\n",
    "            print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation at epoch {epoch}: {e}\")\n",
    "\n",
    "        clear_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Make sure to import your custom classes and functions\n",
    "\n",
    "# Define the path to your saved models\n",
    "save_path = cwd + \"/content/checkpoints\"\n",
    "\n",
    "# List all model files\n",
    "model_files = [f for f in os.listdir(save_path) if f.endswith('.pth')]\n",
    "\n",
    "# Initialize dataset and dataloader for evaluation\n",
    "eval_root_dir = cwd + '/data/sets/nuscenes'\n",
    "eval_version = 'v1.0-mini'\n",
    "eval_split = 'val'\n",
    "radar_channel = 'RADAR_FRONT'\n",
    "img_size = 608\n",
    "batch_size = 8  # Ensure batch size is suitable for evaluation\n",
    "\n",
    "eval_dataset = NuScenesRadarDataset(\n",
    "    root_dir=eval_root_dir,\n",
    "    version=eval_version,\n",
    "    set=eval_split,\n",
    "    radar_channel=radar_channel\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=eval_dataset.collate_fn\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(model_path, device, img_size):\n",
    "    # Load the model\n",
    "    model = Darknet(model_def, img_size=img_size).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Evaluate\n",
    "    precision, recall, AP, f1, ap_class = evaluate(\n",
    "        model,\n",
    "        iou_thres=0.4,\n",
    "        conf_thres=0.05,\n",
    "        nms_thres=0.04,\n",
    "        img_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    return precision.mean(), recall.mean(), AP.mean(), f1.mean()\n",
    "\n",
    "# Evaluate each model and keep track of the best one\n",
    "best_model = None\n",
    "best_mAP = 0.0\n",
    "results = []\n",
    "\n",
    "for model_file in model_files:\n",
    "    model_path = os.path.join(save_path, model_file)\n",
    "    print(f\"Evaluating {model_path}...\")\n",
    "    \n",
    "    precision, recall, mAP, f1 = evaluate_model(model_path, device, img_size)\n",
    "    results.append((model_file, precision.item(), recall.item(), mAP.item(), f1.item()))\n",
    "    \n",
    "    if mAP > best_mAP:\n",
    "        best_mAP = mAP\n",
    "        best_model = model_path\n",
    "    \n",
    "    # Clear GPU memory after each model evaluation\n",
    "    clear_gpu_memory()\n",
    "\n",
    "# Print results\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for result in results:\n",
    "    model_file, precision, recall, mAP, f1 = result\n",
    "    print(f\"{model_file}: Precision: {precision:.4f}, Recall: {recall:.4f}, mAP: {mAP:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "print(f\"Best mAP: {best_mAP:.4f}\")\n",
    "\n",
    "# Define the path for saving results\n",
    "results_path = cwd + \"/content/evaluation_results.csv\"\n",
    "\n",
    "# Write results to a CSV file\n",
    "with open(results_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Model File\", \"Precision\", \"Recall\", \"mAP\", \"F1\"])\n",
    "    for result in results:\n",
    "        writer.writerow(result)\n",
    "    writer.writerow([\"Best Model\", best_model])\n",
    "    writer.writerow([\"Best mAP\", best_mAP])\n",
    "    \n",
    "print(f\"Evaluation results saved to {results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.427 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-218_MAP-0.00.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3532/3282110380.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.942 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:05<00:00,  1.86it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 581.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-184_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.452 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  2.92it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1615.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-230_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.443 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  2.84it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1340.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-238_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.458 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  3.22it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1469.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-194_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.448 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:05<00:00,  2.15it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 728.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-186_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.449 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:04<00:00,  2.55it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 877.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-226_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.449 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:05<00:00,  2.18it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 675.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-202_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.439 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:05<00:00,  2.06it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 623.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-276_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.449 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:04<00:00,  2.41it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 884.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-236_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.442 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  3.17it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1998.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-232_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.452 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:09<00:00,  1.18it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 421.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-256_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.445 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  2.87it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1097.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-252_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.445 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  2.82it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1129.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-248_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.444 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:04<00:00,  2.39it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 951.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-286_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.441 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  2.75it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1321.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-250_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.444 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:07<00:00,  1.49it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 458.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-296_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.440 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:04<00:00,  2.50it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 897.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-274_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.444 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:04<00:00,  2.52it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 979.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-254_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.439 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  3.13it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1864.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-196_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.438 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:16<00:00,  1.49s/it]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 361.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-234_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.440 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:04<00:00,  2.55it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1036.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-246_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.434 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:09<00:00,  1.17it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 393.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-272_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.444 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:04<00:00,  2.23it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 759.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-258_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.434 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:05<00:00,  2.06it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 709.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-270_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.432 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:05<00:00,  2.13it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 582.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-278_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.441 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:39<00:00,  3.56s/it]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 159.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-214_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.438 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:07<00:00,  1.42it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 407.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-198_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.430 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  2.82it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1447.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-228_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.439 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:06<00:00,  1.78it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 615.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-182_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.435 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:06<00:00,  1.68it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 474.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-188_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.427 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:06<00:00,  1.75it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 510.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-264_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.435 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:12<00:00,  1.10s/it]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 358.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-260_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.431 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  2.93it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1579.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-192_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.432 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:05<00:00,  2.03it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 858.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-294_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.430 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:05<00:00,  2.12it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 914.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-216_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.433 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:05<00:00,  1.88it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 563.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-282_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.428 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:04<00:00,  2.33it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 813.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-200_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.431 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  2.82it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1350.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-290_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.430 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:04<00:00,  2.46it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 943.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-298_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.428 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  2.81it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 999.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-268_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.430 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  2.90it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1900.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-224_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.424 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [01:21<00:00,  7.43s/it]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 106.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-220_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.438 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:04<00:00,  2.43it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 891.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-288_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.429 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:08<00:00,  1.34it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 701.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-204_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.428 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [01:17<00:00,  7.05s/it]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 217.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-244_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.431 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:25<00:00,  2.31s/it]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 165.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-222_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.398 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:35<00:00,  3.21s/it]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 318.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-284_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.423 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:08<00:00,  1.34it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 469.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-206_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.430 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:07<00:00,  1.40it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 470.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-266_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.430 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:04<00:00,  2.22it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1132.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-208_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.426 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:21<00:00,  1.96s/it]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 203.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-292_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.436 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:07<00:00,  1.55it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 651.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-212_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.426 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:03<00:00,  2.93it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 1482.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-240_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.433 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:12<00:00,  1.17s/it]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 357.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-262_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.427 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:23<00:00,  2.11s/it]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 330.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-210_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.427 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:07<00:00,  1.52it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 451.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "Evaluating /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-242_MAP-0.00.pth...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.424 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:22<00:00,  2.03s/it]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 224.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU\n",
      "\n",
      "Evaluation Results:\n",
      "yolov3_model_epoch-218_MAP-0.00.pth: Precision: 0.0017, Recall: 0.0032, mAP: 0.0000, F1: 0.0023\n",
      "yolov3_model_epoch-184_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-230_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-238_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-194_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-186_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-226_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-202_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-276_MAP-0.00.pth: Precision: 0.0013, Recall: 0.0016, mAP: 0.0000, F1: 0.0014\n",
      "yolov3_model_epoch-236_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-232_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-256_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-252_MAP-0.00.pth: Precision: 0.0030, Recall: 0.0016, mAP: 0.0000, F1: 0.0021\n",
      "yolov3_model_epoch-248_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-286_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-250_MAP-0.00.pth: Precision: 0.0006, Recall: 0.0016, mAP: 0.0000, F1: 0.0009\n",
      "yolov3_model_epoch-296_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-274_MAP-0.00.pth: Precision: 0.0038, Recall: 0.0032, mAP: 0.0000, F1: 0.0035\n",
      "yolov3_model_epoch-254_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-196_MAP-0.00.pth: Precision: 0.0003, Recall: 0.0016, mAP: 0.0000, F1: 0.0006\n",
      "yolov3_model_epoch-234_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-246_MAP-0.00.pth: Precision: 0.0014, Recall: 0.0048, mAP: 0.0001, F1: 0.0022\n",
      "yolov3_model_epoch-272_MAP-0.00.pth: Precision: 0.0012, Recall: 0.0016, mAP: 0.0000, F1: 0.0014\n",
      "yolov3_model_epoch-258_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-270_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-278_MAP-0.00.pth: Precision: 0.0002, Recall: 0.0016, mAP: 0.0000, F1: 0.0003\n",
      "yolov3_model_epoch-214_MAP-0.00.pth: Precision: 0.0005, Recall: 0.0016, mAP: 0.0000, F1: 0.0008\n",
      "yolov3_model_epoch-198_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-228_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-182_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-188_MAP-0.00.pth: Precision: 0.0007, Recall: 0.0016, mAP: 0.0000, F1: 0.0010\n",
      "yolov3_model_epoch-264_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-260_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-192_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-294_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-216_MAP-0.00.pth: Precision: 0.0014, Recall: 0.0032, mAP: 0.0000, F1: 0.0020\n",
      "yolov3_model_epoch-282_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-200_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-290_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-298_MAP-0.00.pth: Precision: 0.0021, Recall: 0.0016, mAP: 0.0000, F1: 0.0018\n",
      "yolov3_model_epoch-268_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-224_MAP-0.00.pth: Precision: 0.0006, Recall: 0.0080, mAP: 0.0000, F1: 0.0011\n",
      "yolov3_model_epoch-220_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-288_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-204_MAP-0.00.pth: Precision: 0.0005, Recall: 0.0032, mAP: 0.0000, F1: 0.0008\n",
      "yolov3_model_epoch-244_MAP-0.00.pth: Precision: 0.0013, Recall: 0.0096, mAP: 0.0000, F1: 0.0022\n",
      "yolov3_model_epoch-222_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-284_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-206_MAP-0.00.pth: Precision: 0.0020, Recall: 0.0048, mAP: 0.0000, F1: 0.0028\n",
      "yolov3_model_epoch-266_MAP-0.00.pth: Precision: 0.0013, Recall: 0.0016, mAP: 0.0000, F1: 0.0015\n",
      "yolov3_model_epoch-208_MAP-0.00.pth: Precision: 0.0005, Recall: 0.0032, mAP: 0.0000, F1: 0.0008\n",
      "yolov3_model_epoch-292_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-212_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-240_MAP-0.00.pth: Precision: 0.0005, Recall: 0.0016, mAP: 0.0000, F1: 0.0007\n",
      "yolov3_model_epoch-262_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-210_MAP-0.00.pth: Precision: 0.0000, Recall: 0.0000, mAP: 0.0000, F1: 0.0000\n",
      "yolov3_model_epoch-242_MAP-0.00.pth: Precision: 0.0003, Recall: 0.0016, mAP: 0.0000, F1: 0.0004\n",
      "\n",
      "Best Model: /home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-246_MAP-0.00.pth\n",
      "Best mAP: 0.0001\n",
      "Evaluation results saved to /home/synergy-carla/Documents/imen m/yolo/content/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import gc\n",
    "# Import your model and evaluate functions\n",
    "# Evaluation dataset\n",
    "eval_root_dir = cwd+'/data/sets/nuscenes'\n",
    "eval_version = 'v1.0-mini'\n",
    "eval_split = 'val'\n",
    "def clear_gpu_memory():\n",
    "    print(\"Cleared GPU\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "eval_dataset = NuScenesRadarDataset(\n",
    "    root_dir=eval_root_dir,\n",
    "    version=eval_version,\n",
    "    set=eval_split,\n",
    "    radar_channel=radar_channel\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=eval_dataset.collate_fn\n",
    ")\n",
    "# Define the path to your saved models\n",
    "save_path = cwd + \"/content/checkpoints\"\n",
    "\n",
    "# List all model files\n",
    "model_files = [f for f in os.listdir(save_path) if f.endswith('.pth')]\n",
    "\n",
    "# Initialize the model\n",
    "model_def = \"./content/complex_yolov3.cfg\"\n",
    "img_size = 608\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(model_path, device, img_size):\n",
    "    # Load the model\n",
    "    model = Darknet(model_def, img_size=img_size).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Evaluate\n",
    "    precision, recall, AP, f1, ap_class = evaluate(\n",
    "        model,\n",
    "        iou_thres=0.4,\n",
    "        conf_thres=0.05,\n",
    "        nms_thres=0.04,\n",
    "        img_size=img_size,\n",
    "        batch_size=8,\n",
    "    )\n",
    "    \n",
    "    return precision.mean(), recall.mean(), AP.mean(), f1.mean()\n",
    "\n",
    "# Evaluate each model and keep track of the best one\n",
    "best_model = None\n",
    "best_mAP = 0.0\n",
    "results = []\n",
    "\n",
    "for model_file in model_files:\n",
    "    model_path = os.path.join(save_path, model_file)\n",
    "    print(f\"Evaluating {model_path}...\")\n",
    "    \n",
    "    precision, recall, mAP, f1 = evaluate_model(model_path, device, img_size)\n",
    "    results.append((model_file, precision.item(), recall.item(), mAP.item(), f1.item()))\n",
    "    \n",
    "    if mAP > best_mAP:\n",
    "        best_mAP = mAP\n",
    "        best_model = model_path\n",
    "    clear_gpu_memory()\n",
    "\n",
    "# Print results\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for result in results:\n",
    "    model_file, precision, recall, mAP, f1 = result\n",
    "    print(f\"{model_file}: Precision: {precision:.4f}, Recall: {recall:.4f}, mAP: {mAP:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "print(f\"Best mAP: {best_mAP:.4f}\")\n",
    "\n",
    "# Define the path for saving results\n",
    "results_path = cwd + \"/content/evaluation_results.csv\"\n",
    "\n",
    "# Write results to a CSV file\n",
    "with open(results_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Model File\", \"Precision\", \"Recall\", \"mAP\", \"F1\"])\n",
    "    for result in results:\n",
    "        writer.writerow(result)\n",
    "    writer.writerow([\"Best Model\", best_model])\n",
    "    writer.writerow([\"Best mAP\", best_mAP])\n",
    "    \n",
    "print(f\"Evaluation results saved to {results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Evaluating Best Model ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3532/2119480536.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.433 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting objects: 100%|| 11/11 [00:09<00:00,  1.17it/s]\n",
      "Computing AP: 100%|| 2/2 [00:00<00:00, 362.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+---------+\n",
      "| Index | Class name | AP      |\n",
      "+-------+------------+---------+\n",
      "| 0     | pedestrian | 0.00000 |\n",
      "| 1     | vehicle    | 0.00016 |\n",
      "+-------+------------+---------+\n",
      "---- mAP 8.050452807007114e-05\n",
      "Evaluation results saved to /home/synergy-carla/Documents/imen m/yolo/content/best_model_evaluation_results.csv\n",
      "Cleared GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "from terminaltables import AsciiTable\n",
    "import gc\n",
    "\n",
    "# Define the path for the best model\n",
    "best_model_path =\"/home/synergy-carla/Documents/imen m/yolo/content/checkpoints/yolov3_model_epoch-246_MAP-0.00.pth\"\n",
    "img_size = 608  # Ensure this matches your model's expected input size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model(model_path, device, img_size):\n",
    "    # Load the model\n",
    "    model = Darknet(model_def, img_size=img_size).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Evaluate\n",
    "    precision, recall, AP, f1, ap_class = evaluate(\n",
    "        model,\n",
    "        iou_thres=0.4,\n",
    "        conf_thres=0.05,\n",
    "        nms_thres=0.04,\n",
    "        img_size=img_size,\n",
    "        batch_size=8,\n",
    "    )\n",
    "    \n",
    "    return precision, recall, AP, f1, ap_class\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"\\n---- Evaluating Best Model ----\")\n",
    "try:\n",
    "    precision, recall, AP, f1, ap_class = evaluate_model(best_model_path, device, img_size)\n",
    "    \n",
    "    # Print class APs and mAP\n",
    "    ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
    "    for i, c in enumerate(ap_class):\n",
    "        ap_table.append([c, class_names[c], \"%.5f\" % AP[i]])\n",
    "    print(AsciiTable(ap_table).table)\n",
    "    print(f\"---- mAP {AP.mean()}\")\n",
    "\n",
    "    # Define the path for saving results\n",
    "    results_path = cwd + \"/content/best_model_evaluation_results.csv\"\n",
    "    \n",
    "    # Write results to a CSV file\n",
    "    with open(results_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Model File\", \"Precision\", \"Recall\", \"mAP\", \"F1\"])\n",
    "        writer.writerow([best_model_path, precision.mean().item(), recall.mean().item(), AP.mean().item(), f1.mean().item()])\n",
    "        writer.writerow([\"Best Model\", best_model_path])\n",
    "        writer.writerow([\"Best mAP\", AP.mean().item()])\n",
    "        \n",
    "    print(f\"Evaluation results saved to {results_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation of best model: {e}\")\n",
    "\n",
    "finally:\n",
    "    clear_gpu_memory()  # Clear GPU memory\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3.10 (yolo-env)",
   "language": "python",
   "name": "yolo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
